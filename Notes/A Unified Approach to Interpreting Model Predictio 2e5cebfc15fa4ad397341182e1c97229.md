# A Unified Approach to Interpreting Model Predictions

Author: Scott M. Lundberg
DataSet: Private
Status: Not started
Task: Explainability and interpretability
Type of paper: Experimental article

Objective:

Problematic:

- Understanding why a model makes a certain prediction can be as crucial as the prediction’s accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another.

Result:

Quote:

- Understanding why a model makes a certain prediction can be as crucial as the prediction’s accuracy in many applications.
- However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability.
- The ability to correctly interpret a prediction model’s output is extremely important. It engenders appropriate user trust, provides insight into how a model may be improved, and supports understanding of the process being modeled.
- In some applications, simple models (e.g., linear models) are often preferred for their ease of interpretation, even if they may be less accurate than complex ones.
- the growing availability of big data has increased the beneﬁts of using complex models, so bringing to the forefront the trade-off between accuracy and interpretability of a model’s output.
-