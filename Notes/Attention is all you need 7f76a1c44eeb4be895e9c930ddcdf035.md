# Attention is all you need

Author: Ashish Vaswani
Score: ⭐️⭐️⭐️⭐️⭐️
Link: https://github.com/tensorflow/tensor2tensor
DataSet: Public
Date published: 20/08/2017
Status: In progress
Journal Name: Conference on Neural Information Processing Systems 2017
Type of paper: AI-Experiment, Experimental article

Objective:

- In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output.

Quotes:

- Recurrent neural networks, long short-term memory [12] and gated recurrent [7] neural networks in particular, have been ﬁrmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [29, 2, 5].