# An Explainable AI Approach for Breast Cancer Metastasis Prediction Based on Clinicopathological Data

Author: Ikram Maouche
Link: https://github.com/IkramMaouche/CS-CatBoost
DataSet: public dataset you can get from Github
Date published: 21/11/2023
Key word: Bibliometric analysis, Breast cancer, Explainable artiﬁcial intelligence, Machine Learning, clinicopathological, cost sensitive, metastasis
Method: CatBoost   + LIME
Pre-processing: MissForest, The second step consists of undersampling the majority class using Edited Nearest Neighbour (ENN) method [44] to eliminate noisy and overlapping regions.
Status: In progress
Task: Not categorized
Type: Journal
Data type : Clinicopathological data
Journal Name: indexed in IEEE
Features selection : Genetic Algorithm for features selection
Type of paper: Experimental article

Objective:

- Develop an Explainable CAD system to diagnosis metastasis breast cancer. The explainable model could help to increase the complexity the model and allow to interpret the the result, and raise the doctors trust.

Problematic:

- Computer-aided prognosis systems using machine learning models have been widely used to
predict breast cancer metastasis. Despite that, these systems still face several challenges.
    - First, the models are generally biased toward the majority class due to datasets unbalance.
    - Second, their increased complexity is associated with decreased interpretability which causes clinicians to distrust their prognosis.

Task:

Results:

- We used a public dataset of 716 breast cancer patients to assess our approach. The results
demonstrate the superiority of cost-sensitive CatBoost in precision (76.5%), recall (79.5%), and f1-score (77%) over classical and boosting models.
- The LIME explainer was used to quantify the impact of patient and treatment characteristics on breast cancer metastasis, revealing that they have different impacts ranging from high impact like the non-use of adjuvant chemotherapy, and moderate impact including carcinoma with medullary features histological type, to low impact like oral contraception use.

Conclusion:

- This approach could help clinicians understand the factors behind metastasis and assist them in proposing more patient-speciﬁc therapeutic decisions
- 

Techniques:

- for missing data:
    - MissForest [43] to impute missing data. MissForest is a
    non-parametric imputation technique that can be applied to both categorical and continuous data types, making it very suitable for medical data commonly coded as categorical variables. The process starts by replacing the missing values with median values. Then for each column, the rows are split into a test set containing the rows with missing values, while the rest are considered the training set. The sets are fed into a RandomForest model trained to predict the values. The generated prediction for that row is then ﬁlled in to produce the new transformed dataset.
- under-sampling method:
    - The second step consists of under-sampling the majority class using Edited Nearest Neighbour (ENN) method [44] to eliminate noisy and overlapping regions.

Quote:

- Breast Cancer is the most prevalent cancer and the ﬁrst cause of cancer deaths among
women worldwide. In 90% of the cases, mortality is related to distant metastasis.
- In 90% of the cases, mortality is related to the metastasis of tumour cells to other parts of the body [3]. Breast cancer metastasis (BCM) is deﬁned by the spread of cancerous cells to the lymph nodes (loco-regional metastasis) or other organs (distant metastasis) of the body [4]. It is stated that this type of cancer preferentially metastasises to the bone and lungs and less frequently to other organs, including the liver and brain [5].
- The explosive number of electronic health records available nowadays led to the birth of Computer-Aided Prognosis (CAP) models in the healthcare ﬁeld.
- **Model robustness is another important point that CAP models should be able to deliver. The data in real-world are susceptible to disturbances (perturbations), and the CAP models are supposed to produce unperturbed decisions even when the input data is perturbed.**
- **Due to such limit, clinicians are unlikely to trust the results generated by a black-box model nor build their judgements based on its result.**
- Ultimately, model explainability and robustness are the keys of trust and reliability which guarantee that clinicians maintain control [14]. For this reason, the AI community have oriented recently to Explainable Artiﬁcial Intelligence (XAI) [15] to create robust, trustworthy, explainable, and transparent solutions which can gain public trust and accelerate its adoption
in real-world [16].